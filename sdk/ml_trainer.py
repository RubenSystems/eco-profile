# -*- coding: utf-8 -*-
"""IHACK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v2p_5wGN8wEEtfJ-HsohnrrxRHLCqYz9

AUTOENCODER
"""

import tensorflow as tf
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt


# "/content/gdrive/MyDrive/dataset/dataset.csv"

def load_data(filename: str): 
    dataset = pd.read_csv(filename)

    dataset["AEP_MW"] = dataset["AEP_MW"].map(lambda x: x / 1000.0)

    return dataset

    
raw_dataset = load_data("dataset.csv")

"""TRANSFORMER """

x_size = 50
y_size = 50


def get_sliding_windows_transformer(data):

    X = []
    y = []

    for index in range(len(data) - (x_size + y_size) ):
        X.append([m["AEP_MW"] for m in data[index:index + x_size].to_dict(orient="records") ])
        y.append([m["AEP_MW"] for m in data[index + x_size:(index + x_size) + y_size].to_dict(orient="records") ])
    
    return np.array(X), np.array(y)

dataset = get_sliding_windows_transformer(raw_dataset)

x, y = dataset

x.shape

class StepEncoder(tf.keras.layers.Layer):
    def __init__(
        self, num_patches, projection_dim, **kwargs
    ):
        super().__init__(**kwargs)
        self.num_patches = num_patches
        self.position_embedding = tf.keras.layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )
        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)

    def call(self, encoded_patches):
        encoded_positions = self.position_embedding(self.positions)
        encoded_patches = encoded_patches + encoded_positions
        return encoded_patches

class Encoder(tf.keras.layers.Layer):
    def __init__(
            self, input_shape, head_size, num_heads, ff_dim, dropout
        ):
        super().__init__()

        self.mha = tf.keras.layers.MultiHeadAttention(
            key_dim=head_size, num_heads=num_heads, dropout=dropout
        )
        self.drop1 = tf.keras.layers.Dropout(dropout)
        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.add1 = tf.keras.layers.Add()

        self.conv1 = tf.keras.layers.Conv1D(
            filters=ff_dim, kernel_size=1, activation="relu"
        )
        self.drop2 = tf.keras.layers.Dropout(dropout)
        self.conv2 = tf.keras.layers.Conv1D(
            filters=input_shape[-1], kernel_size=1
        )
        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.add2 = tf.keras.layers.Add()

    def call(self, input):
        x = self.mha(input, input)
        x = self.drop1(x)
        x = self.norm1(x)
        res = self.add1([x, input])

        # FFN 
        x = self.conv1(res)
        x = self.drop2(x)
        x = self.conv2(x)
        x = self.norm2(x)
        return self.add2([x, res])

class TSP(tf.keras.Model):
    def __init__(self, 
        input_shape,
        head_size,
        num_heads,
        ff_dim,
        num_transformer_blocks,
        mlp_units,
        dropout=0,
        mlp_dropout=0):

        super().__init__()

        self.embedder = StepEncoder(50, 1)

        self.encoders = [
            Encoder(input_shape, head_size, num_heads, ff_dim, dropout) 
            for _ in range(num_transformer_blocks)
        ]
        self.pool = tf.keras.layers.GlobalAveragePooling1D(
            data_format="channels_first"
        )

        self.mlp = []
        for dim in mlp_units:
            self.mlp.append(tf.keras.layers.Dense(dim, activation="relu"))
            self.mlp.append(tf.keras.layers.Dropout(mlp_dropout))
    

        self.model = tf.keras.Sequential([
            self.embedder,
            *self.encoders,
            self.pool,
            *self.mlp,
            tf.keras.layers.Dense(y_size, activation="linear")
        ])

    def call(self, input):
        return self.model(input)

x = x.reshape(121173, 50, 1)
y = y.reshape(121173, 50, 1)

model = TSP(
    (0, 200),
    head_size=256,
    num_heads=4,
    ff_dim=4,
    num_transformer_blocks=6,
    mlp_units=[128],
    mlp_dropout=0.4,
    dropout=0.25,
)

model.compile(
    loss="mean_squared_error",
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4)
)

model.fit(
    x,
    y,
    epochs=4,
    batch_size=64,
)

model.save('saved/model')

print(x[0])

from sklearn import metrics

pred = model.predict(x)
# score = np.sqrt(metrics.mean_squared_error(pred,y))
# print("Score (RMSE): {}".format(score))

pred.shape

print(pred.shape)
p_c = np.concatenate(pred)
y_c = np.concatenate(y)


plt.plot(p_c, color = 'r')
plt.plot(y_c, color="g")


plt.rcParams["figure.figsize"] = (25,10)


plt.show()